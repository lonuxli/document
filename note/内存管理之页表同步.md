# 内存管理之页表同步

分析平台：x86\-64

内核版本：Linux 4.9

**一、什么是页表同步、为何需要页表同步**

内核空间可以分成下面几种：

1、    directed mapped memory region，就是mapping到main memory的那段虚拟地址空间，VM和PM之间有一个固定的offset。这个memory region的页表是初始化建立的，是不会动态变化的。

2、    permanent kernel mapping、temporary kernel mapping和noncontiguous memory allocation（就是大家熟悉的vmalloc适用的memory region）。这个段的页表可以被更新。

针对上面第二种场景，vmalloc/ioremap 分配的虚拟地址是建立在init\_mm页表中，，但是通常内核态进程上下文，使用的页表通常是进程的有效页表，访存时，有效页表此时还未建立映射关系，所以需要从主页表init\_mm同步。

```
static __always_inline struct rq *
context_switch(struct rq *rq, struct task_struct *prev,
               struct task_struct *next, struct pin_cookie cookie)
{
        struct mm_struct *mm, *oldmm;

        prepare_task_switch(rq, prev, next);

        mm = next->mm;
        oldmm = prev->active_mm;

        if (!mm) {  //内核线程使用前一个运行线程的页表
                next->active_mm = oldmm;
                atomic_inc(&oldmm->mm_count);
                enter_lazy_tlb(oldmm, next);
        } else
                switch_mm_irqs_off(oldmm, mm, next); //用户线程使用自己的进程页表

        if (!prev->mm) {
                prev->active_mm = NULL;
                rq->prev_mm = oldmm;
        }

        /* Here we just switch the register state and the stack. */
        switch_to(prev, next, prev);
        barrier();

        return finish_task_switch(prev);
}
```

vmalloc分配内存是在主页表上建立映射关系，ioremap也是如此。

```
static int vmap_pud_range(pgd_t *pgd, unsigned long addr,
                unsigned long end, pgprot_t prot, struct page **pages, int *nr)
{
        pud_t *pud;
        unsigned long next;

        pud = pud_alloc(&init_mm, pgd, addr);  //vmalloc建立映射时，是在主页表init_mm上建立的
        if (!pud)
                return -ENOMEM;
        do {
                next = pud_addr_end(addr, end);
                if (vmap_pmd_range(pud, addr, next, prot, pages, nr))
                        return -ENOMEM;
        } while (pud++, addr = next, addr != end);
        return 0;
}
```

**二、分配时同步**

1、fork时进程pgd空间分配

dup\_mm\(\)\-\>mm\_init\-\>mm\_alloc\_pgd\(\)\-\>pgd\_alloc\(\)

```
pgd_t *pgd_alloc(struct mm_struct *mm)
{
        pgd_t *pgd;
        pmd_t *pmds[PREALLOCATED_PMDS];

        pgd = _pgd_alloc();

        if (pgd == NULL)
                goto out;

        mm->pgd = pgd;

        if (preallocate_pmds(mm, pmds) != 0)
                goto out_free_pgd;

        if (paravirt_pgd_alloc(mm) != 0)
                goto out_free_pmds;

        /*
         * Make sure that pre-populating the pmds is atomic with
         * respect to anything walking the pgd_list, so that they
         * never see a partially populated pgd.
         */
        spin_lock(&pgd_lock);

        pgd_ctor(mm, pgd); //内核地址空间的页表拷贝
        pgd_prepopulate_pmd(mm, pgd, pmds);

        spin_unlock(&pgd_lock);

        return pgd;

out_free_pmds:
        free_pmds(mm, pmds);
out_free_pgd:
        _pgd_free(pgd);
out:
        return NULL;
}

static void pgd_ctor(struct mm_struct *mm, pgd_t *pgd)
{
        /* If the pgd points to a shared pagetable level (either the
           ptes in non-PAE, or shared PMD in PAE), then just copy the
           references from swapper_pg_dir. */
        if (CONFIG_PGTABLE_LEVELS == 2 ||
            (CONFIG_PGTABLE_LEVELS == 3 && SHARED_KERNEL_PMD) ||
            CONFIG_PGTABLE_LEVELS == 4) {
                clone_pgd_range(pgd + KERNEL_PGD_BOUNDARY, //进程创建时从init_mm同步拷贝页表
                                swapper_pg_dir + KERNEL_PGD_BOUNDARY,
                                KERNEL_PGD_PTRS);
        }

        /* list required to sync kernel mapping updates */
        if (!SHARED_KERNEL_PMD) {
                pgd_set_mm(pgd, mm);
                pgd_list_add(pgd);
        }
}
```

2、缺页异常时pgd表项同步

通常情况下，不同进程的页表的vmalloc地址区域，其pgd都是在fork时分配，但是pud/pmd/pte表项都是共享的，所有页表同步时只需同步pgd表项。

```
static noinline int vmalloc_fault(unsigned long address)
{
        pgd_t *pgd, *pgd_ref;
        pud_t *pud, *pud_ref;
        pmd_t *pmd, *pmd_ref;
        pte_t *pte, *pte_ref;

        /* Make sure we are in vmalloc area: */
        if (!(address >= VMALLOC_START && address < VMALLOC_END))
                return -1;

        WARN_ON_ONCE(in_nmi());

        /*
         * Copy kernel mappings over when needed. This can also
         * happen within a race in page table update. In the later
         * case just flush:
         */
        pgd = (pgd_t *)__va(read_cr3()) + pgd_index(address);
        pgd_ref = pgd_offset_k(address);
        if (pgd_none(*pgd_ref))
                return -1;

        if (pgd_none(*pgd)) {
                set_pgd(pgd, *pgd_ref);  //页表同步的核心实pgd表项的拷贝
                arch_flush_lazy_mmu_mode();
        } else {
                BUG_ON(pgd_page_vaddr(*pgd) != pgd_page_vaddr(*pgd_ref));
        }

        /*
         * Below here mismatches are bugs because these lower tables
         * are shared: 低级页表是共享的，以下都是表项检查
         */

        pud = pud_offset(pgd, address);
        pud_ref = pud_offset(pgd_ref, address);
        if (pud_none(*pud_ref))
                return -1;

        if (pud_none(*pud) || pud_pfn(*pud) != pud_pfn(*pud_ref))
                BUG();

        if (pud_large(*pud))
                return 0;

        pmd = pmd_offset(pud, address);
        pmd_ref = pmd_offset(pud_ref, address);
        if (pmd_none(*pmd_ref))
                return -1;

        if (pmd_none(*pmd) || pmd_pfn(*pmd) != pmd_pfn(*pmd_ref))
                BUG();

        if (pmd_large(*pmd))
                return 0;

        pte_ref = pte_offset_kernel(pmd_ref, address);
        if (!pte_present(*pte_ref))
                return -1;

        pte = pte_offset_kernel(pmd, address);

        /*
         * Don't use pte_page here, because the mappings can point
         * outside mem_map, and the NUMA hash lookup cannot handle
         * that:
         */
        if (!pte_present(*pte) || pte_pfn(*pte) != pte_pfn(*pte_ref))
                BUG();

        return 0;
}
```

**三、释放时同步**

对于vmalloc区域虚拟地址的vmalloc分配和ioremap，有通过缺页异常进行页表同步，那对于vfree和iounmap如何进行页表同步

vfree释放/iounmap时，仅仅是清除pte表项

1、首先、对于给定的虚拟地址，其映射的物理页表不同，同pgd\-pud\-pmd\-pte，这其中只有pte需要修改即可映射到其他物理页面

2、由于所有的页表包括主页表init\_mm共享pud到pte的低级页表，因此只要在主页表中清除pte，再在其他页表中访问已释放的虚拟地址，必然会缺页异常。

```
#define pgd_offset_k(address) pgd_offset(&init_mm, (address))

static void vunmap_page_range(unsigned long addr, unsigned long end)
{
        pgd_t *pgd;
        unsigned long next;

        BUG_ON(addr >= end);
        pgd = pgd_offset_k(addr);  //从init_mm主页表释放
        do {
                next = pgd_addr_end(addr, end);
                if (pgd_none_or_clear_bad(pgd))
                        continue;
                vunmap_pud_range(pgd, addr, next);
        } while (pgd++, addr = next, addr != end);
}
...一层层下去...最终清除的只是pte表项
static void vunmap_pte_range(pmd_t *pmd, unsigned long addr, unsigned long end)
{
        pte_t *pte;

        pte = pte_offset_kernel(pmd, addr);
        do {
                pte_t ptent = ptep_get_and_clear(&init_mm, addr, pte);  //清除pte表项
                WARN_ON(!pte_none(ptent) && !pte_present(ptent));
        } while (pte++, addr += PAGE_SIZE, addr != end);
}
```

**四、kmap为何不需要同步**

kmap使用是4M的虚拟地址大小，其pgd表项在系统启动时就已经设置好，后续的fork都会拷贝这个pgd表项到自身的页表中。

而kmap的使用过程中，只是将虚拟地址映射到不同的物理地址，只需用修改pte表项即可，不会涉及到页表同步的问题。

参考资料：

[http://www.wowotech.net/forum/viewtopic.php?id=27](http://www.wowotech.net/forum/viewtopic.php?id=27)
