# å¹¶å‘åŒæ­¥ä¹‹å†…å­˜å±éšœ

åŸºäºå¹³å°ï¼šARM64

**ä¸€ã€å±éšœæ˜¯ä»€ä¹ˆ**

**1.1 ä¸ºä»€ä¹ˆéœ€è¦å†…å­˜å±éšœ**

```
CPUs and other devices in a system can use a variety of tricks to improve performance, including reorderingï¼ˆä¹±åºï¼‰, deferralï¼ˆå»¶è¿Ÿï¼‰ and combinationï¼ˆåˆå¹¶ï¼‰ of memory operations; speculative loadsï¼ˆéšæœºåŠ è½½ï¼‰; speculative branch predictionï¼ˆéšæœºåˆ†æ”¯é¢„æµ‹ï¼‰ and various types of cachingï¼ˆå„ç§ç±»å‹çš„cacheè¡Œä¸ºï¼‰.Â Â Memory barriers are used to override or suppress these tricks, allowing the code to sanely control the interaction of multiple CPUs and/or devices.Â 
Â Â  Â Â Â  Â Â Â  Â Â Â  Â Â Â  Â Â Â  Â Â Â  Â Â Â  Â Â Â  Â Â Â  Â Â Â  Â Â  Â  â€”â€”â€”â€”<memory-barriers.txt>
```

ä¹±åºçš„ç›´æ¥åŸå› 

å•æ ¸ä¸‹æŒ‡ä»¤ä¹±åºï¼š

1ã€ç¼–è¯‘é˜¶æ®µï¼šç¨‹åºä¸å­˜åœ¨å› æœå…³ç³»æˆ–è€…å­˜åœ¨éšå¼çš„å› æœå…³ç³»ï¼Œç¼–è¯‘å™¨ä¼˜åŒ–æŒ‡ä»¤é¡ºåºå¯¼è‡´ä¹±åº

2ã€æ‰§è¡Œé˜¶æ®µï¼šcpuå†…éƒ¨æŒ‡ä»¤ä¸æ˜¯ä¸¥æ ¼ä¸²è¡Œæ‰§è¡Œï¼Œå¹¶è¡Œæ‰§è¡Œæé«˜æ•ˆç‡ï¼ŒæŒ‡ä»¤æ‰§è¡Œçš„è€—æ—¶ä¸ä¸€è‡´ï¼Œå¯¼è‡´æŒ‡ä»¤æ‰§è¡Œæ—¶ä¹±åº

å¤šæ ¸ä¸‹å†…å­˜åŒæ­¥ï¼š

1ã€é€Ÿåº¦å·®å¼‚ï¼šæ•°æ®åœ¨ä¸åŒæ ¸ä¸Šcacheå‘ˆç°çŠ¶æ€é€ æˆçš„è¯»å–é€Ÿåº¦å·®å¼‚å…ˆåä¹±åº

2ã€è™šå‡å“åº”ï¼šcacheé¡¹invalidè™šå‡å“åº”å¯¼è‡´çš„æ•°æ®ä¸ä¸€è‡´ä¹±åº

![8d0a006771ebe2ad48fd9168fbb94dca.png](image/8d0a006771ebe2ad48fd9168fbb94dca.png)

**1.2Â å†…å­˜å±éšœç±»å‹**

\(1\) Write \(or store\) memory barriers.

\(2\) Data dependency barriers.

\(3\) Read \(or load\) memory barriers.

\(4\) General memory barriers.

\(5\)Â implicit varieties\(ACQUIRE operations/RELEASE operations\)

**1.3Â ä¹±åºç±»å‹**

**1.3.1Â ç¼–è¯‘ä¹±åº**

ç•¥

**1.3.2 å†…å­˜ä¹±åº**

ä¸åŒæ¶æ„ä¸‹å­˜åœ¨çš„å†…å­˜ä¹±åºâ€”â€”\<perfbook2.pdf\>

![eff443232bb7ee0dabb48b0c7ccada72.png](image/eff443232bb7ee0dabb48b0c7ccada72.png)

é€šè¿‡å¯¹ä¸Šè¡¨åˆ†ç±»ï¼Œå¯ä»¥æ€»ç»“å‡ºå­˜åœ¨memoryÂ reorderingçš„å‡ ç§æƒ…å†µåŠä¸åŒå¹³å°æ‰€å…·å¤‡çš„ç‰¹å¾ï¼š

**å†…å­˜ä¹±åºç±»å‹ï¼š**

1ã€Load/StoreÂ ReorderedÂ AfterÂ Load/Store

ä¸Šè¡¨memoryÂ orderingä¸­1\-3è¡Œï¼Œæè¿°ä¸ç›¸å…³åœ°å€çš„å†…å­˜ä¹±åºé—®é¢˜

2ã€Atomic Instructions Reordered

ä¸Šè¡¨memoryÂ orderingä¸­ç¬¬4è¡Œï¼Œ"indicates whether a given CPUÂ allows loads and stores to be reordered with atomic instructions." atomicå†…å­˜è®¿é—®æ“ä½œå’Œæ™®é€šçš„load/storeç›¸äº’é—´å­˜åœ¨ä¹±åºé—®é¢˜ã€‚

3ã€Dependent Loads/Store Reordered

ä¸Šè¡¨memoryÂ orderingä¸­5\-6è¡Œï¼Œæè¿°ä¸¤ä¸ªè®¿é—®åœ°å€åœ°å€é—´å­˜åœ¨data/address/control dependencyå…³ç³»æ—¶çš„å†…å­˜ä¹±åºé—®é¢˜ã€‚

4ã€Non\-Sequentially Consistent

ä¸Šè¡¨memoryÂ orderingä¸­ç¬¬7è¡Œã€‚æš‚æ—¶ä¸ç†è§£

5ã€MutilcopyÂ Atomic

ä¸Šè¡¨memoryÂ orderingä¸­8\-9è¡Œã€‚æè¿°å˜é‡åˆ°è¾¾ä¸åŒcpuæ ¸çš„é€Ÿåº¦å·®å¼‚ï¼Œå¯¼è‡´å†…å­˜ä¹±åºçš„é—®é¢˜ã€‚

6ã€Cache Coherent

ä¸Šè¡¨memoryÂ orderingä¸­ç¬¬10è¡Œï¼Œæš‚æ—¶ä¸ç†è§£

**äºŒã€å†…æ ¸ä¸­å±éšœçš„å®ç°**

**2.1Â barrier\(\)**

**2.1.1Â å®ç°åŸç†**

barrier\(\)æœ¬è´¨æ˜¯GNUÂ Cæ‰©å±•æ±‡ç¼–å±‚é¢çš„å®ç°ï¼Œä»¥ä¸Šæ±‡ç¼–ä»£ç ç”¨äºæŠ‘åˆ¶ç¼–è¯‘å™¨çš„ä¼˜åŒ–è€Œä¸ç”Ÿæˆæ–°æŒ‡ä»¤ä»£ç ã€‚

```
#define barrier() __asm__ __volatile__("": : :"memory")
```

volatile:è¡¨ç¤ºæŒ‡ç¤ºç¼–è¯‘å™¨å¯¹è¯¥åµŒå…¥æ±‡ç¼–ä»£ç ä¸è¿›è¡Œä»£ç ä¼˜åŒ–ï¼Œä¿®æ”¹æŒ‡ä»¤é¡ºåºç­‰ï¼Œ\_\_asm\_\_ \_\_volatile\_\_å¯ä»¥ç”¨asmå’Œvolatileï¼Œä¸ºäº†é¿å…å’ŒCå…³é”®å­—é‡å¤è­¦å‘Šï¼Œæœ€å¥½åŠ ä¸‹åˆ’çº¿.

memoryï¼šæ˜¯GCCæ‰©å±•æ±‡ç¼– clobber list\(ç ´åç¬¦åˆ—è¡¨\)ä¸­çš„ä¸€ä¸ªå…³é”®å­—ï¼Œè¯¥åˆ—è¡¨ç›®çš„æ˜¯å‘Šè¯‰ç¼–è¯‘å™¨è¿™æ®µä»£ç å¯¹æŸäº›å†…å­˜äº§ç”Ÿç ´åä¿®æ”¹ï¼Œå› æ­¤memoryæ„ä¹‰ä¸ºï¼š

1ã€å‘Šè¯‰ç¼–è¯‘å™¨è¯¥æ®µä»£ç ä¼šä¿®æ”¹å†…å­˜ï¼Œå…¶å¼ºåˆ¶ç¼–è¯‘å™¨åœ¨è¯¥æ®µä»£ç å‰ä¿å­˜ç¼“å­˜å€¼ï¼ˆå¯„å­˜å™¨å€¼ï¼‰ï¼Œåœ¨è¯¥æ®µä»£ç ä¹‹åé‡æ–°åŠ è½½å˜é‡ï¼ˆä»å†…å­˜ä¸­åŠ è½½è€Œä¸æ˜¯ä»å¯„å­˜å™¨ç¼“å­˜ä¸­åŠ è½½ï¼‰

2ã€å‘Šè¯‰ç¼–è¯‘å™¨åœ¨è¯¥æ®µä»£ç å‰åï¼Œéœ€è¦ä¿è¯æŒ‡ä»¤çš„é¡ºåºï¼ˆæ˜¯æ‰€æœ‰æŒ‡ä»¤è¿˜æ˜¯åªæ˜¯å†…å­˜è®¿é—®ç›¸å…³çš„æŒ‡ä»¤ï¼Ÿï¼‰

```
The next statement creates a special clobber to tell the compiler, that memory contents may have changed. Again, the clobber list will be explained later, when we take a look to code optimization.

It tells the compiler that the assembler instruction may change memory locations. This forces the compiler to store all cached values before and reload them after executing the assembler instructions. And it must retain the sequence, because the contents of all variables is unpredictable after executing an asm statement with a memory clobber.

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â â€”â€”â€”â€”<ARM GCC inline Assembler.pdf>
```

**2.1.2Â ä½¿ç”¨åœºæ™¯**

1ã€æŠ‘åˆ¶ç¼–è¯‘å™¨é€ æˆçš„æŒ‡ä»¤é‡æ’

```
int a, b;

//ä»¥ä¸‹ç¼–è¯‘å™¨è®¤ä¸ºa/bçš„èµ‹å€¼åœ¨å•çº¿ç¨‹è§†è§’ä¸‹æ²¡æœ‰ç›¸å…³æ€§ï¼Œå¯èƒ½å°†å¯¹bçš„èµ‹å€¼æ—©äºaï¼Œå› æ­¤å¯èƒ½é€ æˆå¤šçº¿ç¨‹åœºæ™¯ä¸‹çš„é€»è¾‘é”™è¯¯ã€‚
void foo(void)
{
Â Â Â Â Â Â Â Â a = b + 1;
Â Â Â Â Â Â Â Â b = 0;
}
//é€šè¿‡ä½¿ç”¨barrieræ¥æŠ‘åˆ¶è¿™ç§ç¼–è¯‘é€ æˆçš„ä¹±åºï¼Œåˆ©ç”¨ä¸Šä¸€èŠ‚ä¸­æ‰©å±•æ±‡ç¼–ä¸­memoryçš„ä½œç”¨2
void foo(void)
{
Â Â Â Â Â Â Â Â a = b + 1;
Â Â Â Â Â Â Â Â barrier();
Â  Â  Â  Â  b = 0;
}
```

2ã€æŠ‘åˆ¶å†…å­˜ä¼˜åŒ–åˆå¹¶å†…å­˜æ“ä½œ

```
int run = 1;

//å¦‚ä¸‹ä»£ç åœ¨å¾ªç¯ä¸­åˆ¤æ–­runçš„å€¼ï¼Œè€Œrunå¯èƒ½åœ¨å…¶ä»–çº¿ç¨‹ä¸­è¢«ä¿®æ”¹
void foo(void)
{
Â Â Â Â Â Â Â Â while (run);
}
//è€Œç¼–è¯‘å™¨åœ¨å•çº¿ç¨‹è§†è§’ä¸‹ï¼Œæå–å¯¹runå€¼çš„è¯»å–ï¼Œå‡å°‘å†…å­˜è®¿é—®æ˜¯å¯ä»¥ä¼˜åŒ–æ€§èƒ½çš„ï¼Œä½†æ˜¯è¿™ç§ä¼˜åŒ–å¹¶ä¸æ˜¯å·¥ç¨‹å¸ˆæƒ³è¦çš„
void foo(void)
{
Â Â Â Â Â Â Â Â register int reg = run;
Â Â Â Â Â Â Â Â if (reg) while (1);
}
//é€šè¿‡ä½¿ç”¨barrierï¼Œä½¿å¾—ç¼–è¯‘è®¤ä¸ºæ¯æ¬¡å¾ªç¯å†…å­˜éƒ½å‘ç”Ÿå˜åŒ–ï¼Œå› æ­¤éœ€è¦åœ¨æ¯ä¸ªå¾ªç¯ä¸­é‡æ–°é‡å†…å­˜ä¸­è¯»å–runå€¼ï¼Œè¿™æ˜¯åˆ©ç”¨äº†ä¸Šä¸€èŠ‚æ‰©å±•æ±‡ç¼–memoryçš„ä½œç”¨1
void foo(void)
{Â  Â  Â  Â Â 
Â Â  Â Â Â  Â while (run)
Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â barrier();
}
```

**2.2** **mb\(\)**

**2.2.1Â å®ç°åŸç†**

ä¸è®ºåœ¨å•æ ¸è¿˜æ˜¯å¤šæ ¸åœºæ™¯ä¸‹ï¼Œmbçš„å®ç°ç›¸åŒï¼ŒåŸºäºdsbæŒ‡ä»¤å®ç°

```
#define mb()Â Â Â Â Â Â Â Â Â Â Â Â dsb(sy)
#define rmb()Â Â Â Â Â Â Â Â Â Â Â dsb(ld)
#define wmb()Â Â Â Â Â Â Â Â Â Â Â dsb(st)

#define dsb(opt)Â Â Â Â Â Â Â Â asm volatile("dsb " #opt : : : "memory")
```

DSBæŒ‡ä»¤ä½œç”¨ï¼ˆæŒ‡ä»¤å…·ä½“çš„å½±å“ä½œç”¨èŒƒå›´éœ€å–å†³äºoptionï¼‰ï¼š

1ã€ç¡®ä¿dsbæŒ‡ä»¤å‰åçš„Load/StoreæŒ‡ä»¤å†…å­˜è®¿é—®æ“ä½œä¸ä¹±åº

2ã€ç¡®ä¿dsbæŒ‡ä»¤åå…¶ä»–æ›´å¤šçš„æŒ‡ä»¤è¢«é˜»éš”ï¼ŒçŸ¥é“dsbæŒ‡ä»¤å‰çš„åŒæ­¥å®Œæˆ

3ã€ç¡®ä¿dsbæŒ‡ä»¤å‰çš„cache, TLB and branch predictor maintenance operationså®Œæˆä¸é˜»éš”

```
This enforces the same ordering as the Data Memory Barrier, but has the additional effect of blocking execution of any further instructions, not just loads or stores, or both, until synchronization is complete. This can be used to prevent execution of a SEV instruction, for instance, that would signal to other cores that an event occurred. It waits until all cache, TLB and branch predictor maintenance operations issued by this processor have completed for the specified shareability domain.
Â Â  Â Â Â  Â Â  Â Â Â  Â Â Â  Â Â Â  Â Â Â  Â Â Â  Â Â Â  Â Â Â  Â Â Â  Â Â Â  Â  â€”â€”â€”â€”<ARM_v8_architecture_Programmer Guide v1.0.pdf>
```

**2.2.2Â ä½¿ç”¨åœºæ™¯**

1ã€mb\(\)å±éšœæ˜æ˜¾æ¯”smp\_mb\(\)è¦†ç›–çš„èŒƒå›´æ›´å¹¿æ›´å…¨é¢ï¼Œå› æ­¤èƒ½ä½¿ç”¨smp\_mb\(\)çš„åœ°æ–¹å¿…ç„¶å¯ä»¥ä½¿ç”¨mb\(\)ï¼Œä½†æ˜¯ä¹Ÿä¼šå› æ­¤é€ æˆå¯¹CPUæ€§èƒ½çš„è¿‡åº¦æŠ‘åˆ¶ã€‚

2ã€å†…å­˜æ“ä½œçš„è§‚å¯Ÿè€…æ¶‰åŠCPUå’Œç¡¬ä»¶è®¾å¤‡ï¼ˆä¾‹å¦‚ç½‘ç»œè®¾å¤‡ï¼ŒDMAï¼‰ï¼Œéœ€ä½¿ç”¨\[w,r\]mb\(\)ï¼Œå› ä¸ºè¿™ç§åœºæ™¯ä¸‹å³ä½¿æ˜¯å•æ ¸ä¹Ÿéœ€è¦ä¿è¯å†…å­˜ä¸€è‡´æ€§ã€‚

**2.3Â smp\_mb\(\)**

**2.3.1Â å•æ ¸å®ç°**

åœ¨å•æ ¸éSMPåœºæ™¯ä¸‹ï¼Œå±éšœå®ç°é€€åŒ–ä¸ºbarrier\(\)

```
#define smp_mb()Â Â Â Â Â Â Â Â barrier()
#define smp_rmb()Â Â Â Â Â Â Â barrier()
#define smp_wmb()Â Â Â Â Â Â Â barrier()
```

**2.3.2Â å¤šæ ¸å®ç°**

åœ¨å¤šæ ¸SMPåœºæ™¯ä¸‹ï¼Œå±éšœåŸºäºdmbæŒ‡ä»¤å®ç°

```
#define smp_mb()Â Â Â Â Â Â Â Â __smp_mb()
#define smp_rmb()Â Â Â Â Â Â Â __smp_rmb()
#define smp_wmb()Â Â Â Â Â Â Â __smp_wmb()

#define __smp_mb()Â Â Â Â Â Â dmb(ish)
#define __smp_rmb()Â Â Â Â Â dmb(ishld)
#define __smp_wmb()Â Â Â Â Â dmb(ishst)

#define dmb(opt)Â Â Â Â Â Â Â Â asm volatile("dmb " #opt : : : "memory")
```

**2.3.2 DMBæŒ‡ä»¤**

1ã€ç¡®ä¿dmbæŒ‡ä»¤å‰åçš„Load/storeæŒ‡ä»¤å†…å­˜è®¿é—®æ“ä½œä¸ä¹±åº

2ã€ç¡®ä¿dmbæŒ‡ä»¤å‰çš„cacheÂ maintenanceÂ operationså®Œæˆä¸é˜»éš”ï¼ŒcacheæŒ‡çš„æ˜¯CPUæ ¸ä¸å†…å­˜é—´çš„cache

```
This prevents re-ordering of data accesses instructions across the barrier instruction. All data accesses, that is, loads or stores, but not instruction fetches, performed by this processor before the DMB, are visible to all other masters within the specified shareability domain before any of the data accesses after the DMB.

It also ensures that any explicit preceding data or unified cache maintenance operations have completed before any subsequent data accesses are executed.
Â Â  Â Â Â  Â Â Â  Â Â Â  Â Â Â  Â Â Â  Â Â Â  Â Â Â  Â Â Â  Â Â  Â Â Â  Â Â Â  Â Â â€”â€”â€”â€”<ARM_v8_architecture_Programmer Guide v1.0.pdf>
```

**2.3.4Â ä½¿ç”¨åœºæ™¯**

1ã€ä»smp\_mbçš„å®ç°æ¥çœ‹ï¼Œå³è¡¨æ˜å…¶åªæœ‰åœ¨smpå¤šæ ¸ä¸‹æ‰æœ‰æ•ˆã€‚

2ã€è§£å†³å¤šæ ¸é—´å…±äº«å˜é‡å› CPUå†…å­˜ä¹±åºï¼Œé€ æˆçš„é€»è¾‘å¼‚å¸¸é—®é¢˜ï¼Œè¿™ç§åœºæ™¯ä¸‹å†…å­˜è®¿é—®çš„è§‚å¯Ÿè€…å‡ä¸ºCPUæ ¸ã€‚åˆ©ç”¨äº†å¦‚ä¸ŠdmbæŒ‡ä»¤çš„ç¬¬ä¸€ç‚¹ä½œç”¨ã€‚

_ARM__v8ä¸­isb/dmb/dsbæŒ‡ä»¤å·®å¼‚_

_1ã€ç¡®ä¿isbä¹‹åçš„æŒ‡ä»¤é‡æ–°é¢„å–_

_2ã€åœ¨ç¡¬ä»¶å±‚é¢çœ‹æ¥ï¼Œæ‰§è¡Œisbä¹‹åï¼ŒinstructionÂ pipelineÂ isÂ flushed_

_3ã€åœ¨å¦‚ä¸‹åœºæ™¯ä¸‹éœ€è¦ä½¿ç”¨ï¼šmemoryÂ managementï¼ŒcacheÂ controlï¼ŒcontextÂ switchingï¼ŒcodeÂ isÂ movedÂ aboutÂ inÂ memory_

**2.4 smp\_read\_barrier\_depends**

**2.4.1Â å®ç°åŸç†**

å•æ ¸éSMPåœºæ™¯

```
#define smp_read_barrier_depends()Â Â Â Â Â Â do { } while (0)
```

å¤šæ ¸SMPåœºæ™¯

```
#define smp_read_barrier_depends()Â Â Â Â Â Â __smp_read_barrier_depends()
#define __smp_read_barrier_depends()Â Â Â Â read_barrier_depends()

//éalphaæ¶æ„ï¼Œæœ¬è´¨ä¸ºç©ºå®ç°
#define read_barrier_depends() do { } while (0)
//åªæœ‰alphaæ¶æ„å®šä¹‰äº†
#define read_barrier_depends() __asm__ __volatile__("mb": : :"memory")
```

smp\_read\_barrier\_depends\(\)è¢«ç§°ä¸ºdata dependency barrierã€‚

smp\_read\_barrier\_depends\(\) å®åœ¨linuxå†…æ ¸ä¸­çš„æ„ä¹‰æ˜¯ï¼šsmp\_read\_barrier\_dependsåŸè¯­ä¿è¯å±éšœå‰çš„è¯»å†…å­˜æ“ä½œåœ¨å±éšœåçš„è¯»å†…å­˜æ“ä½œ_ï¼ˆè¯¥è¯»å†…å­˜æ“ä½œçš„åœ°å€æˆ–æ•°æ®ä¾èµ–äºå‰é¢è¯»æ“ä½œçš„è¿”å›å€¼ï¼‰_å‰å®Œæˆã€‚è¿™æ˜¯ä¸€ç§è½»äºrmb\(\)å±éšœçš„åŸè¯­ï¼Œrmb\(\)å±éšœå¯¹å‰åè¯»æ˜¯å¦å­˜åœ¨ä¾èµ–å…³ç³»å¹¶ä¸å…³å¿ƒï¼Œå³ä½¿åè€…è¯»ä¸ä¾èµ–å‰è€…è¯»çš„ç»“æœä¹Ÿèƒ½ä¿è¯é¡ºåºã€‚

```
Flush all pending reads that subsequents reads depend onã€‚
All reads preceding this primitive are guaranteed to access memory (but not necessarily other CPUs' caches) before any reads following this primitive that depend on the data return by any of the preceding reads.Â Â This primitive is much lighter weight than rmb() on most CPUs, and is never heavier weight than is rmb().
Â Â  Â Â Â  Â Â Â  Â Â Â  Â Â Â  Â Â Â  Â Â Â  Â Â Â  Â Â  Â  â€”â€”â€”â€”<arch/alpha/include/asm/barrier.h>
```

åœ¨Linux 4.14\-rc7åŠ å…¥çš„è¡¥ä¸\<Add implicit smp\_read\_barrier\_depends\(\) to READ\_ONCE\(\)\>76ebbe78f7390aee075a7f3768af197ded1bdfbbå°†smp\_read\_barrier\_dependsé›†æˆåˆ°READ\_ONCE\(\)å®å†…ï¼Œç›¸åº”çš„åŸå…ˆä½¿ç”¨smp\_read\_barrier\_depends\(\)çš„ä»£ç éƒ½ä½¿ç”¨READ\_ONCE\(\)æ›¿ä»£ï¼Œæ¥è§„èŒƒè¿™äº›å±éšœåŸè¯­çš„ä½¿ç”¨ã€‚

```
#define __READ_ONCE(x, check)Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â \
({Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â \
Â Â Â Â Â Â Â Â union { typeof(x) __val; char __c[1]; } __u;Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â \
Â Â Â Â Â Â Â Â if (check)Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â \
Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â __read_once_size(&(x), __u.__c, sizeof(x));Â Â Â Â Â Â Â Â Â Â Â Â Â \
Â Â Â Â Â Â Â Â elseÂ Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â \
Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â __read_once_size_nocheck(&(x), __u.__c, sizeof(x));Â Â Â Â Â \
Â Â Â Â Â Â Â Â smp_read_barrier_depends(); /* Enforce dependency ordering from x */ \
Â Â Â Â Â Â Â Â __u.__val;Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â \
})
#define READ_ONCE(x) __READ_ONCE(x, 1)
```

**2.4.2Â alphaæ¶æ„ä»£ç å®ç°**

```
#define read_barrier_depends() __asm__ __volatile__("mb": : :"memory")
#define mb()Â Â Â Â __asm__ __volatile__("mb": : :"memory")
#define rmb()Â Â Â __asm__ __volatile__("mb": : :"memory")
#define wmb()Â Â Â __asm__ __volatile__("wmb": : :"memory")
```

smp\_read\_barrier\_depends\(\)åœ¨Linuxå†…æ ¸ä¸­çš„å®šä¹‰æœ¬åº”è¯¥åªç”¨äºaddress dependencesåœºæ™¯ä¸‹ä½¿ç”¨çš„è½»é‡çº§å±éšœï¼Œå…¶ä¸èƒ½é‡äºrmb\(\)æ“ä½œï¼Œåœ¨alphaä¸­å…¶å®ç°æ˜¯ç­‰åŒäºrmb\(\)çš„ã€‚è™½ç„¶ä¸¤è€…å®ç°ç›¸åŒï¼Œä½†æ˜¯åœ¨ä¸ç›¸å…³åœ°å€load\-loadä¹±åºä¸­è¿˜æ˜¯è¦ä½¿ç”¨rmb\(\)ï¼Œä¸è¦å½¢å¼ä¸Šçš„é€ æˆæ··ä¹±ã€‚

alphaæ¶æ„ä¸­mbæŒ‡ä»¤çš„ä½œç”¨è¯´æ˜ï¼š

```
mb:Guarantee that all subsequent loads or stores will not access memory until after all previous loads and stores have accessed memory, as
observed by other processors.Â  Â Â  Â  â€”â€”â€”â€”<alpha_arch_ref.pdf>
```

**2.4.3** **smp\_read\_barrier\_depends****ä½¿ç”¨åœºæ™¯**

smp\_read\_barrier\_depends\(\)è™½ç„¶è¢«ç§°ä¸ºdata dependency barrierã€‚æ ¹æ®ã€Šperfbook 15.2.3èŠ‚ã€‹å…¶é€‚ç”¨äºçš„å…·ä½“åœºæ™¯æ˜¯åœ°å€ä¾èµ–å…³ç³»\(Address Dependencies\)ä¸­çš„Load\-Loadä¾èµ–ã€‚address dependencyæ¡ä»¶æˆç«‹å¯ä»¥ä¿è¯é™¤alphaå¤–çš„å…¶ä»–æ‰€æœ‰CPUæ¶æ„çš„Load\-Load &Â Load\-Storeä¸ä¹±åºï¼Œä»¥åŠaddress dependencyåªèƒ½ä¿è¯alphaæ¶æ„çš„Load\-Storeä¸ä¹±åºã€‚ä»¥ä¸‹æ˜¯ä¸¤ç§Address Dependenciieså…³ç³»æ¨¡å‹ï¼š

```
Load-LoadÂ Â  Â Â Â  Â Â Â  Â Â Â  Â Â  Â  Load-Store

//step1:è·å–æŒ‡é’ˆÂ Â  Â Â Â  Â Â Â  Â Â  //step1:è·å–æŒ‡é’ˆ
Q = READ_ONCE(P);Â Â  Â Â Â  Â Â Â  Â Q = READ_ONCE(P);
//step2:è¯»æŒ‡é’ˆæŒ‡å‘å†…å­˜Â  Â Â Â Â  Â //step2:å†™æŒ‡é’ˆæŒ‡å‘å†…å­˜
D = *Q;Â Â  Â Â Â  Â Â Â  Â Â Â  Â Â Â  Â Â  WRITE_ONCE(*Q, 2)
```

åœ¨åœ°å€ä¾èµ–Load\-LoadÂ æ¨¡å‹æƒ…å†µä¸‹ï¼Œå¦‚ä¸‹ç¤ºä¾‹ä»£ç ä¼šå‡ºç°D==2å¹¶ä¸”Q==&Bå¼‚å¸¸æƒ…å†µï¼Œalphaå¹³å°æ— æ³•ä¿è¯CPU1çš„æ„ŸçŸ¥ä¹±åºï¼Œä»¿ä½›CPU1ä¸Šå­˜åœ¨Load\-Loadä¹±åºã€‚å› æ­¤éœ€è¦åœ¨CPU1ä¸Šçš„ä¸¤ä¸ªloadä¹‹é—´å¢åŠ è¯»ä¾èµ–å±éšœï¼Œå…³äºé€ æˆä¹±åºçš„åŸå› ï¼Œmemory\-barriers.txtä¸­ä¹Ÿæœ‰è§£é‡Š:Â åŸå› ä¸ºCPUæ¶æ„ä½¿ç”¨äº†machines with split cachesã€‚

```
Â Â Â Â Â Â Â Â CPU 1Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â CPU 2
Â Â Â Â Â Â Â Â ===============Â Â Â Â Â Â Â ===============
Â Â Â Â Â Â Â Â { A == 1, B == 2, C == 3, P == &A, Q == &C }
Â Â Â Â Â Â Â Â B = 4;
Â Â Â Â Â Â Â Â <write barrier>
Â Â Â Â Â Â Â Â WRITE_ONCE(P, &B);
Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Q = READ_ONCE(P);
Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â D = *Q;

Note that this extremely counterintuitive situation arises most easily on machines with split caches, so that, for example, one cache bank processes even-numbered cache lines and the other bank processes odd-numbered cache lines.Â Â The pointer P might be stored in an odd-numbered cache line, and the variable B might be stored in an even-numbered cache line.Â Â Then, if the even-numbered bank of the reading CPU's cache is extremely busy while the odd-numbered bank is idle, one can see the new value of the pointer P (&B), but the old value of the variable B (2)
Â Â  Â Â Â  Â Â Â  Â Â Â  Â Â Â  Â Â Â  Â Â Â  Â Â Â  Â Â Â  Â Â Â  Â Â Â  Â Â Â  Â Â Â  Â â€”â€”â€”â€”<memory-barriers.txt>
```

**2.4.3Â å…³äºåœ°å€ä¾èµ–Load\-Storeæ€è€ƒ**

åœ¨alphaå¹³å°ä¸Šå­˜åœ¨åœ°å€ä¾èµ–Load\-Loadä¹±åºé—®é¢˜ï¼Œé‚£ä¹ˆä¸ºä½•ä¸å­˜åœ¨Load\-Storeä¹±åºï¼Ÿè§£é‡Šå¦‚ä¸‹ï¼š

```
A data-dependency barrier is not required to order dependent writes because the CPUs that the Linux kernel supports don't do writes until they are certain (1) that the write will actually happen, (2) of the location of the write, and (3) of the value to be written.Â åªæœ‰ç¡®å®šçš„å†™å…¥åœ°å€å’Œå†™å…¥å€¼æ‰ä¼šæ‰§è¡Œå†™æ“ä½œã€‚
Â Â  Â Â Â  Â Â Â  Â Â Â  Â Â Â  Â Â Â  Â Â Â  Â Â Â  Â Â Â  Â Â Â  Â Â Â  Â Â Â  Â â€”â€”â€”â€”<memory-barriers.txt>
```

**2.5Â ACQUIRE/RELEASEéšå¼å†…å­˜å±éšœ**

éšå¼å†…å­˜å±éšœæœ¬è´¨ä¸Šå°è£…äº†å‰é¢çš„\[r,w\]mb\(\)ç­‰ï¼Œå…¶å¯èƒ½ä¸æ˜¯å®Œæ•´çš„memory barrierï¼Œå…·ä½“å–å†³äºå…¶å®ç°ã€‚

**2.5.1Â ACQUIRE operations**

1ã€smp\_load\_acquire\(\)/smp\_cond\_acquire\(\)é€šç”¨å®ç°

```
#define smp_load_acquire(p) __smp_load_acquire(p)
#define __smp_load_acquire(p)Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â \
({Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â \
Â Â Â Â Â Â Â Â typeof(*p) ___p1 = READ_ONCE(*p);Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â \
Â Â Â Â Â Â Â Â compiletime_assert_atomic_type(*p);Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â \
Â Â Â Â Â Â Â Â __smp_mb();Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â \
Â Â Â Â Â Â Â Â ___p1;Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â \
})
```

2ã€å†…æ ¸ä¸­çš„å„ç§lock\(\)æ“ä½œ

**2.5.2Â RELEASE operations**

1ã€smp\_store\_release\(\)é€šç”¨å®ç°

```
#define smp_store_release(p, v) __smp_store_release(p, v)
#define __smp_store_release(p, v)Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â \
do {Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â \
Â Â Â Â Â Â Â Â compiletime_assert_atomic_type(*p);Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â \
Â Â Â Â Â Â Â Â __smp_mb();Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â \
Â Â Â Â Â Â Â Â WRITE_ONCE(*p, v);Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â \
} while (0)
```

2ã€å†…æ ¸ä¸­çš„å„ç§unlock\(\)æ“ä½œ

**2.5.3 smp\_load\_acquire\(\)/smp\_store\_release\(\)è¯´æ˜**

åœ¨Load\-Storeä¹±åºæƒ…å†µä¸‹ä½¿ç”¨smp\_mb\(\)æ˜¯ä¸€ä¸ªæ¯”è¾ƒé‡çš„æ“ä½œï¼Œåœ¨å¾ˆå¤šæ¶æ„ä¸‹æœ‰æ›´è½»é‡çº§æ–¹æ³•ã€‚å› æ­¤å¼•å…¥smp\_load\_acquire\(\)/smp\_store\_release\(\)æ¥å®ç°ä¸åŒå¹³å°å·®å¼‚åŒ–ä¼˜åŒ–ã€‚

smp\_load\_acquire\(\)ï¼šä¿è¯åŸè¯­å‰çš„Loadä¸åŸè¯­åçš„Load/Storeä¸ä¹±åº

smp\_store\_release\(\)ï¼šä¿è¯åŸè¯­å‰çš„Load/Storeä¸åŸè¯­åçš„Storeä¸ä¹±åº

```
A number of situations currently require the heavyweight smp_mb(), even though there is no need to order prior stores against laterÂ  loads.Â Â Many architectures have much cheaper ways to handle these situations, but the Linux kernel currently has no portable way to make use of them.
The new smp_load_acquire() primitive orders the specified load against any subsequent reads or writes, while the new smp_store_release()Â  primitive orders the specifed store against any prior reads or writes.
```

å…·ä½“å‚è€ƒï¼š\<arch: Introduce smp\_load\_acquire\(\), smp\_store\_release\(\)\>Â 47933ad41a86a4a9b50bed7c9b9bd2ba242aac63

**2.5.4Â ARM64ä¸­ldar/stlrè¯´æ˜**

```
An LDAR instruction guarantees that any memory access instructions after the LDAR, are only visible after the load-acquire. A store-release guarantees that all earlier memory accesses are visible before the store-release becomes visible and that the store is visible to all parts of the system capable of storing cached data at the same time.
```

**2.6 smp\_mb\_\_before/after\_atomic**

**2.6.1Â å®ç°åŸç†**

å•æ ¸éSMPåœºæ™¯

```
#define smp_mb__before_atomic() barrier()
#define smp_mb__after_atomic()Â Â barrier()
```

å¤šæ ¸SMPåœºæ™¯

```
#define smp_mb__before_atomic() __smp_mb__before_atomic()
#define smp_mb__after_atomic()Â Â __smp_mb__after_atomic()

//å†…æ ¸é»˜è®¤å®ç°ï¼Œå¦‚ARM64å¹³å°å­˜åœ¨è¿™ç±»ä¹±åº
#define __smp_mb__before_atomic()Â Â Â Â Â Â Â __smp_mb()
#define __smp_mb__after_atomic()Â Â Â Â Â Â Â Â __smp_mb()

//x86ä¸å­˜åœ¨è¿™ç±»ä¹±åºï¼Œæ‰€ä»¥æ˜¯ç©ºå®ç°
/* Atomic operations are already serializing on x86 */
#define __smp_mb__before_atomic()Â Â Â Â Â Â Â do { } while (0)
#define __smp_mb__after_atomic()Â Â Â Â Â Â Â Â do { } while (0)
```

**2.6.2Â åº”ç”¨åœºæ™¯**

smp\_mb\_\_before/after\_atomicç”¨äºæŠ‘åˆ¶atomicæŒ‡ä»¤ä¸load/storeå†…å­˜æŒ‡ä»¤é—´å­˜åœ¨çš„ä¹±åºé—®é¢˜ï¼Œæœ‰äº›CPUæ¶æ„ä¸å­˜åœ¨è¿™ç§ä¹±åºé—®é¢˜ï¼Œå› æ­¤å¼•å…¥è¯¥å±éšœåŸè¯­ï¼Œå®ç°åªæœ‰åœ¨ä¹±åºå¹³å°æ‰æ’å…¥å±éšœæŒ‡ä»¤ã€‚commit:febdbfe8a91ce0d11939d4940b592eb0dba8d663

**2.7Â** **smp\_mb\_\_after\_spinlock**

**2.7.1Â å®ç°åŸç†**

```
//é»˜è®¤å®ç°
#define smp_mb__after_spinlock()Â Â Â Â Â Â Â Â do { } while (0)

//arm64/powerpc/riscv
#define smp_mb__after_spinlock()Â Â Â Â Â Â Â Â smp_mb()
```

**2.7.2Â åº”ç”¨åœºæ™¯**

spin\_lock/unlock\(\)åœ¨ä»£ç å®ç°ä¸Šæ˜¯å¸¦æœ‰acquire/releaseåŸè¯­ï¼Œå¦‚ä¸Šä¸€èŠ‚è¯¥åŸè¯­åœ¨ä¸åŒçš„æ¶æ„å¹³å°ä¸Šå®ç°ä¸ä¸€ï¼Œæœ‰äº›æ¶æ„å¹³å°ä½¿ç”¨çš„æ˜¯ä¸å®Œæ•´çš„è½»é‡çº§çš„å±éšœæŒ‡ä»¤ï¼Œåœ¨æŸäº›æƒ…å†µä¸‹ï¼Œè¿™äº›ä¸å®Œæ•´çš„å±éšœåŸè¯­å¹¶ä¸èƒ½å®Œå…¨ä¿è¯ä¸ä¹±åºã€‚å…·ä½“æƒ…å†µå‚è€ƒ3.4èŠ‚ä¸åŒå¹³å°ç‰¹å¾è¡¨æ ¼ã€‚

æ¡ˆä¾‹1ï¼šspin\_lock\(S\)å¹¶ä¸èƒ½ä¿è¯å‰é¢çš„WRITE\_ONCE\(X, 1\)ä¸ä¹±åºåˆ°åé¢å»ï¼Œå› æ­¤éœ€è¦å¢åŠ ä¸€ä¸ªå®Œæ•´å±éšœ

æ¡ˆä¾‹2ï¼šç±»ä¼¼äº\<perfbook2\>ä¸­çš„Listing 15.16ï¼Œåœ¨non\-multilecopy\-atomicçš„æƒ…å†µä¸‹ï¼Œç”±äºspin\_lock\(S\)å³acquireå¹¶ä¸å…·å¤‡Cumulativity/Propagationï¼Œå› æ­¤éœ€è¦å¢åŠ ä¸€ä¸ªå®Œæ•´çš„å±éšœä¿è¯CPU1è¯»Xæ—¶ï¼ŒCPU2ä¹Ÿèƒ½è·å–åˆ°Xæœ€æ–°å€¼ã€‚Â  Â Â 

```
*Â Â Â 1) Given the snippet:
*
*Â Â Â Â Â Â Â Â { X = 0;Â Â Y = 0; }
*
*Â Â Â Â Â Â Â Â CPU0Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â CPU1
*
*Â Â Â Â Â Â Â Â WRITE_ONCE(X, 1);Â Â Â Â Â Â Â Â Â Â Â Â Â WRITE_ONCE(Y, 1);
*Â Â Â Â Â Â Â Â spin_lock(S);Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â smp_mb();
*Â Â Â Â Â Â Â Â smp_mb__after_spinlock();Â Â Â Â Â r1 = READ_ONCE(X);
*Â Â Â Â Â Â Â Â r0 = READ_ONCE(Y);
*Â Â Â Â Â Â Â Â spin_unlock(S);

*Â Â Â 2) Given the snippet:
*
*Â Â { X = 0;Â Â Y = 0; }
*
*Â Â CPU0Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â CPU1Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â CPU2
*
*Â Â spin_lock(S);Â Â Â Â Â Â Â spin_lock(S);Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â r1 = READ_ONCE(Y);
*Â Â WRITE_ONCE(X, 1);Â Â Â smp_mb__after_spinlock();Â Â Â Â Â Â Â smp_rmb();
*Â Â spin_unlock(S);Â Â Â Â Â r0 = READ_ONCE(X);Â Â Â Â Â Â Â Â Â Â Â Â Â Â r2 = READ_ONCE(X);
*Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â WRITE_ONCE(Y, 1);
*Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â spin_unlock(S);
```

**2.8Â å†…æ ¸å±éšœå®ç°ç‰¹å¾æ€»ç»“**

![fe444905d94230750a4f0a4dcdbfe2b0.png](image/fe444905d94230750a4f0a4dcdbfe2b0.png)

**ä¸‰ã€å†…å­˜å±éšœæ€»ç»“**

3.1 ç¼–è¯‘å™¨ä¼˜åŒ–

**3.2 CPUå†…å­˜ä¹±åºï¼ˆä¸ç›¸å…³åœ°å€åŠåœ°å€ä¾èµ–å…³ç³»ï¼‰**

![Image.png](image/Image.png)

![6c3958c14e559771993c43ef1d1ac336.png](image/6c3958c14e559771993c43ef1d1ac336.png)

**3.3 CPU****å†…å­˜ä¹±åº\(atomicæŒ‡ä»¤ä¸load/storeé—´ä¹±åº\)**

**![6f37d586013c47a6a33de1405cd6c3ab.png](image/6f37d586013c47a6a33de1405cd6c3ab.png)**

**3.4Â å†…å­˜ä¹±åº\(ä¸åŒå¹³å°acquireåŸè¯­å®ç°å·®å¼‚è€Œå¼•å…¥çš„smp\_mb\_\_after\_spinlockä¸€è§ˆè¡¨\)**

![cd47f2aec8a04a72bf9ecd58811c8c85.png](image/cd47f2aec8a04a72bf9ecd58811c8c85.png)

**3.5Â ä¸åŒæ¶æ„çš„å†…å­˜ä¹±åºç‰¹å¾è¡¥å……è¡¨æ ¼**

![memory_order_in_arch.png](image/memory_order_in_arch.png)

**å››ã€å‚è€ƒèµ„æ–™ï¼š**

Documentation/memory\-barriers.txt

[https://yq.aliyun.com/articles/337916](https://yq.aliyun.com/articles/337916)

[https://cloud.tencent.com/developer/article/1006236](https://cloud.tencent.com/developer/article/1006236)

[http://www.wowotech.net/kernel\_synchronization/Why\-Memory\-Barriers.html/comment\-page\-2\#comments](http://www.wowotech.net/kernel_synchronization/Why-Memory-Barriers.html/comment-page-2#comments)
